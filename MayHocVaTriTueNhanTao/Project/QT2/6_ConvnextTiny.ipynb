{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a90e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import from other modules\n",
    "from data_loader import get_cifar100_loaders\n",
    "from evaluation_utils import (evaluate_model, plot_confusion_matrix,\n",
    "                              visualize_misclassified, get_cifar100_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4a78bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs, learning_rate, device, model_name=\"convnext_tiny\"):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.05)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "    model.to(device)\n",
    "    best_accuracy = 0.0\n",
    "    model_save_path = f'{model_name}_cifar100_best.pth'\n",
    "\n",
    "    train_acc_history = []\n",
    "    test_acc_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        train_iterator = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for i, (inputs, labels) in enumerate(train_iterator):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            train_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss_train = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc_train = 100.0 * correct_train / total_train\n",
    "        train_acc_history.append(epoch_acc_train)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        test_iterator = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_iterator:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss_batch = criterion(outputs, labels)\n",
    "                running_val_loss += val_loss_batch.item() * inputs.size(0)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(test_loader.dataset)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        epoch_acc_test = 100.0 * correct_test / total_test\n",
    "        test_acc_history.append(epoch_acc_test)\n",
    "        end_time = time.time()\n",
    "        epoch_duration = end_time - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | '\n",
    "              f'Train Loss: {epoch_loss_train:.4f} | Train Acc: {epoch_acc_train:.2f}% | '\n",
    "              f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_acc_test:.2f}% | '\n",
    "              f'LR: {optimizer.param_groups[0][\"lr\"]:.6f} | '\n",
    "              f'Duration: {epoch_duration:.2f}s')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch_acc_test > best_accuracy:\n",
    "            best_accuracy = epoch_acc_test\n",
    "            try:\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f'>>> Best model saved to {model_save_path} with Val Accuracy: {best_accuracy:.2f}%')\n",
    "            except Exception as e:\n",
    "                 print(f\"Lỗi khi lưu model: {e}\")\n",
    "\n",
    "    print('Finished Training')\n",
    "    print(f'Best Validation Accuracy achieved during training: {best_accuracy:.2f}%')\n",
    "\n",
    "    print(f\"Loading best model state from {model_save_path} for final evaluation...\")\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load best model state: {e}. Evaluating with the final model state.\")\n",
    "\n",
    "    return model, train_acc_history, test_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "777762f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sử dụng thiết bị: cuda\n",
      "Đang tải dữ liệu CIFAR-100 (resize: 224x224)...\n",
      "Đã tải xong CIFAR-100.\n",
      "Số lượng ảnh Train: 50000\n",
      "Số lượng ảnh Test: 10000\n",
      "Kích thước ảnh: 224x224\n",
      "Sử dụng Data Augmentation: True\n",
      "Khởi tạo 6_CONVNEXTTINY pre-trained model...\n",
      "Đã thay thế lớp Linear trong classifier của ConvNeXt_Tiny bằng lớp Linear(768, 100)\n",
      "Bắt đầu fine-tuning 6_CONVNEXTTINY...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mĐã thay thế lớp Linear trong classifier của ConvNeXt_Tiny bằng lớp Linear(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_ftrs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBắt đầu fine-tuning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m trained_model, train_history, test_history \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     40\u001b[0m     model, train_loader, test_loader, NUM_EPOCHS, LEARNING_RATE, device, model_name\u001b[38;5;241m=\u001b[39mMODEL_NAME\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# --- Final Evaluation and Visualization ---\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Performing Final Evaluation on Test Set for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, num_epochs, learning_rate, device, model_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[1;32m      5\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[38;5;241m=\u001b[39mnum_epochs, eta_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m best_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      9\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cifar100_best.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 915\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    919\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1342\u001b[0m         device,\n\u001b[1;32m   1343\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1344\u001b[0m         non_blocking,\n\u001b[1;32m   1345\u001b[0m     )\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001    # ConvNeXt fine-tuning often uses smaller learning rates\n",
    "IMG_SIZE = 224\n",
    "USE_AUGMENTATION = True\n",
    "DATA_DIR = './data_cifar100'\n",
    "NUM_WORKERS = 4\n",
    "MODEL_NAME = \"6_ConvnextTiny\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Sử dụng thiết bị: {device}\")\n",
    "\n",
    "class_names = get_cifar100_class_names(DATA_DIR)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Đang tải dữ liệu CIFAR-100 (resize: {IMG_SIZE}x{IMG_SIZE})...\")\n",
    "train_loader, test_loader, _ = get_cifar100_loaders(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    data_dir=DATA_DIR,\n",
    "    img_size=IMG_SIZE,\n",
    "    use_augmentation=USE_AUGMENTATION,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "if train_loader is None:\n",
    "    print(\"Không thể tải dữ liệu. Kết thúc chương trình.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Khởi tạo {MODEL_NAME.upper()} pre-trained model...\")\n",
    "weights = models.ConvNeXt_Tiny_Weights.DEFAULT\n",
    "model = models.convnext_tiny(weights=weights)\n",
    "\n",
    "# --- Modify the classifier for CIFAR-100 ---\n",
    "num_ftrs = model.classifier[2].in_features\n",
    "model.classifier[2] = nn.Linear(num_ftrs, num_classes)\n",
    "print(f\"Đã thay thế lớp Linear trong classifier của ConvNeXt_Tiny bằng lớp Linear({num_ftrs}, {num_classes})\")\n",
    "\n",
    "print(f\"Bắt đầu fine-tuning {MODEL_NAME.upper()}...\")\n",
    "trained_model, train_history, test_history = train_model(\n",
    "    model, train_loader, test_loader, NUM_EPOCHS, LEARNING_RATE, device, model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "# --- Final Evaluation and Visualization ---\n",
    "print(f\"\\n--- Performing Final Evaluation on Test Set for {MODEL_NAME.upper()} ---\")\n",
    "final_metrics = evaluate_model(trained_model, test_loader, device, num_classes, class_names)\n",
    "\n",
    "if 'confusion_matrix' in final_metrics:\n",
    "    plot_confusion_matrix(final_metrics['confusion_matrix'], class_names,\n",
    "                            filename=f'{MODEL_NAME}_confusion_matrix.png',\n",
    "                            figsize=(25, 25))\n",
    "\n",
    "visualize_misclassified(trained_model, test_loader, device, class_names,\n",
    "                        num_images=25,\n",
    "                        filename_prefix=f'{MODEL_NAME}_misclassified')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), train_history, label='Train Accuracy')\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), test_history, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title(f'{MODEL_NAME.upper()} Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'{MODEL_NAME}_accuracy_history.png')\n",
    "print(f\"Accuracy history plot saved to {MODEL_NAME}_accuracy_history.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
