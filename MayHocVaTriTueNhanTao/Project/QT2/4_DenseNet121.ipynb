{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fafa6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import time\n",
    "from tqdm import tqdm # Direct import\n",
    "import matplotlib.pyplot as plt\n",
    "from data_loader import get_cifar100_loaders\n",
    "from evaluation_utils import (evaluate_model, plot_confusion_matrix,\n",
    "                              visualize_misclassified, get_cifar100_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67198ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs, learning_rate, device, model_name=\"densenet121\"):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Adam or SGD can work for DenseNet; Adam is often a good starting point for fine-tuning.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4) # Add some weight decay\n",
    "    # Consider a learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    model.to(device)\n",
    "    best_accuracy = 0.0\n",
    "    model_save_path = f'{model_name}_cifar100_best.pth'\n",
    "\n",
    "    train_acc_history = []\n",
    "    test_acc_history = []\n",
    "    val_loss_history = [] # For ReduceLROnPlateau\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        train_iterator = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for i, (inputs, labels) in enumerate(train_iterator):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            train_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss_train = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc_train = 100.0 * correct_train / total_train\n",
    "        train_acc_history.append(epoch_acc_train)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        test_iterator = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_iterator:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss_batch = criterion(outputs, labels) # Calculate validation loss per batch\n",
    "                running_val_loss += val_loss_batch.item() * inputs.size(0)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(test_loader.dataset)\n",
    "        val_loss_history.append(epoch_val_loss)\n",
    "        epoch_acc_test = 100.0 * correct_test / total_test\n",
    "        test_acc_history.append(epoch_acc_test)\n",
    "        end_time = time.time()\n",
    "        epoch_duration = end_time - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | '\n",
    "              f'Train Loss: {epoch_loss_train:.4f} | Train Acc: {epoch_acc_train:.2f}% | '\n",
    "              f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_acc_test:.2f}% | ' # Changed \"Test\" to \"Val\"\n",
    "              f'LR: {optimizer.param_groups[0][\"lr\"]:.6f} | ' # Get LR from optimizer\n",
    "              f'Duration: {epoch_duration:.2f}s')\n",
    "\n",
    "        # Step the scheduler with validation loss\n",
    "        scheduler.step(epoch_val_loss)\n",
    "\n",
    "        # Save best model based on validation accuracy\n",
    "        if epoch_acc_test > best_accuracy:\n",
    "            best_accuracy = epoch_acc_test\n",
    "            try:\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f'>>> Best model saved to {model_save_path} with Val Accuracy: {best_accuracy:.2f}%')\n",
    "            except Exception as e:\n",
    "                 print(f\"Lỗi khi lưu model: {e}\")\n",
    "\n",
    "    print('Finished Training')\n",
    "    print(f'Best Validation Accuracy achieved during training: {best_accuracy:.2f}%')\n",
    "\n",
    "    print(f\"Loading best model state from {model_save_path} for final evaluation...\")\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load best model state: {e}. Evaluating with the final model state.\")\n",
    "\n",
    "    return model, train_acc_history, test_acc_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0975af",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 20         # As per previous request\n",
    "BATCH_SIZE = 32         # DenseNet can also be memory intensive, adjust if needed\n",
    "LEARNING_RATE = 0.001\n",
    "IMG_SIZE = 224          # DenseNet pre-trained models also expect 224x224\n",
    "USE_AUGMENTATION = True\n",
    "DATA_DIR = './data_cifar100'\n",
    "NUM_WORKERS = 4\n",
    "MODEL_NAME = \"4_DenseNet121\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Sử dụng thiết bị: {device}\")\n",
    "\n",
    "class_names = get_cifar100_class_names(DATA_DIR)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Đang tải dữ liệu CIFAR-100 (resize: {IMG_SIZE}x{IMG_SIZE})...\")\n",
    "train_loader, test_loader, _ = get_cifar100_loaders(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    data_dir=DATA_DIR,\n",
    "    img_size=IMG_SIZE,\n",
    "    use_augmentation=USE_AUGMENTATION,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "if train_loader is None:\n",
    "    print(\"Không thể tải dữ liệu. Kết thúc chương trình.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Khởi tạo {MODEL_NAME.upper()} pre-trained model...\")\n",
    "# --- Load DenseNet121 with pre-trained weights ---\n",
    "weights = models.DenseNet121_Weights.IMAGENET1K_V1\n",
    "model = models.densenet121(weights=weights)\n",
    "# Or older: model = models.densenet121(pretrained=True)\n",
    "\n",
    "# --- Modify the classifier for CIFAR-100 ---\n",
    "# DenseNet's classifier is a single nn.Linear layer\n",
    "num_ftrs = model.classifier.in_features\n",
    "model.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "print(f\"Đã thay thế lớp classifier của DenseNet121 bằng lớp Linear({num_ftrs}, {num_classes})\")\n",
    "\n",
    "print(f\"Bắt đầu fine-tuning {MODEL_NAME.upper()}...\")\n",
    "trained_model, train_history, test_history = train_model(\n",
    "    model, train_loader, test_loader, NUM_EPOCHS, LEARNING_RATE, device, model_name=MODEL_NAME\n",
    ")\n",
    "\n",
    "# --- Final Evaluation and Visualization ---\n",
    "print(f\"\\n--- Performing Final Evaluation on Test Set for {MODEL_NAME.upper()} ---\")\n",
    "final_metrics = evaluate_model(trained_model, test_loader, device, num_classes, class_names)\n",
    "\n",
    "if 'confusion_matrix' in final_metrics:\n",
    "    plot_confusion_matrix(final_metrics['confusion_matrix'], class_names,\n",
    "                            filename=f'{MODEL_NAME}_confusion_matrix.png',\n",
    "                            figsize=(25, 25))\n",
    "\n",
    "visualize_misclassified(trained_model, test_loader, device, class_names,\n",
    "                        num_images=25,\n",
    "                        filename_prefix=f'{MODEL_NAME}_misclassified')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), train_history, label='Train Accuracy')\n",
    "plt.plot(range(1, NUM_EPOCHS + 1), test_history, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title(f'{MODEL_NAME.upper()} Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'{MODEL_NAME}_accuracy_history.png')\n",
    "print(f\"Accuracy history plot saved to {MODEL_NAME}_accuracy_history.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
